# Robust_RL_for_Algorithmic_Trading_ECE542
Adversarially-Robust Deep Q-Network for Algorithmic Trading
Motivation: Reinforcement learning in the financial sector at a grand scale is greatly affected by the curse of dimensionality, and quite often neural approximations are required. Deep Q-Networks are widely applied in algorithmic trading in such a situation, but market uncertainties—dynamic variability, noise, and crafted adversarial effects—make the problem much harder. This project aims at designing a provably robust Deep Q-Network to enhance reliability in uses of algorithmic trading, in spite of adversarial contaminations in the data..
Data Specification: The Yahoo Finance Dataset(2018-2023) [1] contains daily stock market data for equities, ETFs, and indexes from April 1, 2018, to March 31, 2023, which we collected from Yahoo Finance to analyze market trends, detect patterns, and develop investment strategies. Therefore, we will use this dataset to test and validate our algorithms.
Architecture Type: We would like to examine two architectures, MLP and CNN, for applicability with our neural approximated Q-learning algorithm. While they are provably susceptible [2] to adversarial perturbations, they are critical in detecting temporal patterns from stock data. Our objective is to find out whether our robust algorithms can overcome the weaknesses, making algorithmic trading stable. In recent years, there have been some attempts at making the cited architectures more robust. Be that as it may, let it be noted that, in our case, we use them to estimate our iterates (Q-table) for Deep Q-Networks and do not have a priori knowledge that robust architectures ensure robustness for deep Q-learning. Moreover, too much robustness can hurt performance, so careful design and well-balanced building of our algorithm is necessary.
Plan of Work:
Data Preprocessing: To train our Deep Q-Network, we first need to preprocess our dataset so that we have consistent and good quality data. Naturally occurring missing values occur because of the weekends and holidays, and if not taken care of, these gaps might be misinterpreted by the DQN as sudden market spikes, causing the network to learn inaccurately. Also, undue market movements, can skew the model. Moreover, without proper feature normalization, the DQN could end up weighing some features incorrectly, making biased decisions. These preprocessing tasks are required in order to let the model learn efficiently from clean, well-organized financial data.
Distributed Adversarial Design: Using the preprocessed and cleaned dataset, we will construct an active distributed adversarial attack, motivated by the description in [2]. Since stock patterns are time-sensitive, we conjecture that distributional but mild adversaries have a considerable impact on algorithmic choices.
Conduct Experiments: Next, we need to conduct a series of experiments to determine the impact of adversarial influence on the Deep Q-Learning algorithm in the context of algorithmic trading. Our aim is to demonstrate that even a slight percentage or small magnitude of adversarial perturbations can lead to suboptimal performance, eventually leading to losses in algorithmic trading.
Robust Deep Q-Learning: Finally, we wish to build an Efficient DQN that is not too fragile so that algorithmic trading with DQN still has some strength and ensures high-probability results in our case. Briefly, we need to demonstrate that our robust algorithm can be executed efficiently even with adversary poisoning.
Preliminary research on Robust reinforcement learning  without neural approximations, with finite-time rates and mathematically supported guarantees and bounds of principles, has been conducted by a collaborator at CDC 2024 and AISTATS 2025.
[1] Yahoo Finance Dataset(2018-2023)
[2] Adversarial Perturbation Defense on Deep Neural Networks (Zhang et. al)
